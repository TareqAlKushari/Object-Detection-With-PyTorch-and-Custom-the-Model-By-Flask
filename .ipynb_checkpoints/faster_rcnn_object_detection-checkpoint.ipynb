{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90b13e70-6b8a-4795-ab7d-5b0eb87078b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from coco_names import COCO_INSTANCE_CATEGORY_NAMES as coco_names\n",
    "\n",
    "import detect_utils\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30a55a93-63ba-4d90-abc8-6009ea8ffd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a93c95a6-bd0a-4ba9-ab62-b1e6a18c8a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will help us create a different color for each class\n",
    "COLORS = np.random.uniform(0, 255, size=(len(coco_names), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11e60885-addd-4bab-a7d6-36408dd5e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the torchvision image transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "022afac2-4093-46bf-9525-95948d6f2e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image, model, device, detection_threshold):\n",
    "    # transform the image to tensor\n",
    "    image = transform(image).to(device)\n",
    "    image = image.unsqueeze(0) # add a batch dimension\n",
    "    outputs = model(image) # get the predictions on the image\n",
    "    # print the results individually\n",
    "    # print(f\"BOXES: {outputs[0]['boxes']}\")\n",
    "    # print(f\"LABELS: {outputs[0]['labels']}\")\n",
    "    # print(f\"SCORES: {outputs[0]['scores']}\")\n",
    "    # get all the predicited class names\n",
    "    pred_classes = [coco_names[i] for i in outputs[0]['labels'].cpu().numpy()]\n",
    "    # get score for all the predicted objects\n",
    "    pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "    # get all the predicted bounding boxes\n",
    "    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
    "    # get boxes above the threshold score\n",
    "    boxes = pred_bboxes[pred_scores >= detection_threshold].astype(np.int32)\n",
    "    return boxes, pred_classes, outputs[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24fdd274-3550-4e29-842b-a61ef2f5b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(boxes, classes, labels, image):\n",
    "    # read the image with OpenCV\n",
    "    image = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)\n",
    "    for i, box in enumerate(boxes):\n",
    "        color = COLORS[labels[i]]\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (int(box[0]), int(box[1])),\n",
    "            (int(box[2]), int(box[3])),\n",
    "            color, 2\n",
    "        )\n",
    "        cv2.putText(image, classes[i], (int(box[0]), int(box[1]-5)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2, \n",
    "                    lineType=cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0408b478-0160-45e3-9c82-18d3ab3f66ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-i INPUT] [-m MIN_SIZE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\alKushari\\AppData\\Roaming\\jupyter\\runtime\\kernel-f8a543ee-8187-4d48-be6e-a052ec6b6d47.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# construct the argument parser\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-i', '--input', help='path to input image/video')\n",
    "parser.add_argument('-m', '--min-size', dest='min_size', default=800, help='minimum input size for the FasterRCNN network')\n",
    "args = vars(parser.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b1d6f09-ccbd-4a0f-80ce-aa1ce0783151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(input, min_size=800):\n",
    "    # download or load the model from disk\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, min_size=min_size)\n",
    "    \n",
    "    image = Image.open(input)\n",
    "    model.eval().to(device)\n",
    "    boxes, classes, labels = detect_utils.predict(image, model, device, 0.8)\n",
    "    image = detect_utils.draw_boxes(boxes, classes, labels, image)\n",
    "    cv2.imshow('Image', image)\n",
    "    save_name = f\"{input.split('/')[-1].split('.')[0]}_{min_size}\"\n",
    "    cv2.imwrite(f\"outputs/{save_name}.jpg\", image)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98788b84-edbb-4e8b-9ec7-6aa461670c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download or load the model from disk\n",
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, min_size=args['min_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4cc729b-fcaa-435c-9857-60ebbb7c9cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open(args['input'])\n",
    "# model.eval().to(device)\n",
    "# boxes, classes, labels = detect_utils.predict(image, model, device, 0.8)\n",
    "# image = detect_utils.draw_boxes(boxes, classes, labels, image)\n",
    "# cv2.imshow('Image', image)\n",
    "# save_name = f\"{args['input'].split('/')[-1].split('.')[0]}_{args['min_size']}\"\n",
    "# cv2.imwrite(f\"outputs/{save_name}.jpg\", image)\n",
    "# cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "988d5a31-f444-442a-b6aa-b09ab17c3645",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'JpegImageFile' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# python detect.py --input input/horses.jpg\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m detect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput/horses.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 8\u001b[0m, in \u001b[0;36mdetect\u001b[1;34m(input, min_size)\u001b[0m\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m boxes, classes, labels \u001b[38;5;241m=\u001b[39m detect_utils\u001b[38;5;241m.\u001b[39mpredict(image, model, device, \u001b[38;5;241m0.8\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m image \u001b[38;5;241m=\u001b[39m detect_utils\u001b[38;5;241m.\u001b[39mdraw_boxes(boxes, classes, labels, image)\n\u001b[0;32m      9\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage\u001b[39m\u001b[38;5;124m'\u001b[39m, image)\n\u001b[0;32m     10\u001b[0m save_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\Jupyter\\Advanced Artificial Intelligence\\Projects\\Faster RCNN Object Detection\\detect_utils.py:49\u001b[0m, in \u001b[0;36mdraw_boxes\u001b[1;34m(boxes, classes, labels, image)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_boxes\u001b[39m(boxes, classes, labels, image):\n\u001b[0;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m    Draws the bounding box around a detected object.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     lw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mround\u001b[39m(\u001b[38;5;28msum\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.003\u001b[39m), \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Line width.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     tf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(lw \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Font thickness.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, box \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(boxes):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'JpegImageFile' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# python detect.py --input input/horses.jpg\n",
    "detect(\"input/horses.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e215b0-88ad-4984-9774-9676d8f6832e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920bc061-7296-4eec-8057-5241f4e9c119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed3a39-e460-441a-97a4-59b63a797972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0e43bc-b06e-4cd4-95e7-4c625573b59b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b8f6c9-44a2-4129-a5fb-f7641dac0e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
